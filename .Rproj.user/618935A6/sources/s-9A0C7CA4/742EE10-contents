---
title: "Personnalité"
author: "cb"
date: "19/10/2021"
output: html_document
---

## Objectifs

Comprendre quels sont les facteurs qui sont associés à une faible ou une forte recommandation. Le cas est celui d'une chaine de jardinerie et d'une enquête menée sur sa clientèle dans les années 2010. 

On utilise deux questions principalement :

- une échelle de recommandation (de 0 à 10)
- une question ouverte sur les raison de la note.

Sur la recommandation, c'est la technique du NPS qui est employée. On pourra lire la synthèse de [Daniel Ray](https://www.cairn.info/revue-l-expansion-management-review-2014-3-page-58.htm). On lira [ici ](https://hbr.org/2003/12/the-one-number-you-need-to-grow) l'article original.


## les packages utilisés

On utilise principalement les ressources de `quanteda` et l'analyse factorielle des correspondances avec `Factominer`

```{r setup}
knitr::opts_chunk$set(echo = TRUE,include=TRUE, warning=FALSE, message=FALSE)
library(tidyverse)
library(readr)

library(lubridate)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)

library(ggwordcloud)
library(ggmosaic)
library(rcompanion)

library(cowplot)
library ("FactoMineR")
library(factoextra)
library(seededlda)
library(gam)
```



## lecture et recodage des données

```{r 02}
df<-read.csv("perso.csv") %>% rename(personnalite=4,nps=5)
#palette de couleur
col<-c("firebrick","Gold3","Darkgreen")
theme_set(theme_minimal())
```

## la distribution du nps

générale

```{r 03}

g<-ggplot(df, aes(x=nps))+
  geom_histogram(binwidth = 1, fill="pink")+
  labs( title= " Distribution des scores NPS", 
        subtitle = "", 
        caption = "n=", 
        y = "Fréquence")+ 
  scale_fill_manual(values=col)
g  
ggsave("NPS1.jpg", plot=last_plot(), width = 20, height = 20, units = "cm")

```

```{r 03}

g<-ggplot(df, aes(x=Marque, y= nps))+
  geom_boxplot()+
  labs( title= " Distribution des scores NPS par Marque", 
        subtitle = "", 
        caption = "n=", 
        y = "NPS")+ 
  scale_fill_manual(values=col) + coord_flip()
g  

ggsave("NPS1.jpg", plot=last_plot(), width = 20, height = 20, units = "cm")

```


Comme nous allons traiter du texte et que celui n'est présent que dans une partie des réponses, on peut se demander s'il y a un biais. 

par marque

## Connaissance et expérience



```{r 02b}

#library(ggmosaic)
#library(rcompanion)
#recodons les répondants

foo1 <- foo1 %>% 
  mutate(reponse=ifelse(is.na(explication), "Pas de réponse", "Réponse"))

t<-table(foo1$NPS,foo1$reponse)


chi2<-chisq.test(t)
chi<-round(chi2$statistic,2)
p<-round(chi2$p.value,3)
V<-cramerV(t, digit=3)

g1 <- ggplot(data = foo1) +
  geom_mosaic(aes(x=product(NPS ,reponse), fill = NPS))+  
  theme(axis.text.x = element_text(angle = 45, hjust = -0.1, vjust = -0.2))+ 
  theme(legend.position = "none")+
  labs(title="Un biais négatif de réponse", 
       subtitle=paste0("chi2 =",chi, " p = ", p, " - V : ", V))+    
  scale_fill_manual(values=col) 

g1
```



## Structure des corrélations

On peut examiner la corrélation de cet indicateurs à quelques autres variables : d'autres jugements, et les comportements antérieurs.



```{r 02b}

foo2<-foo1 %>% select(3,4,5,7,8,9) %>% drop_na()
r<- round(cor(foo2),3)
library(ggcorrplot)
ggcorrplot(r, hc.order = TRUE, type = "lower",
   outline.col = "white",
   colors = c("#6D9EC1", "white", "#E46726"),
   lab=TRUE)

library(GGally)
ggpairs(foo2, lower = list(continuous = "smooth_loess", combo = ggally_dot_no_facet),title="correlogram with ggpairs()") 

```

##  Un petit modèle linéaire

```{r cor}
#ggplot(foo1,aes(CaTicket))+geom_histogram(binwidth = 1)
#ggplot(foo1,aes(CaTicket, nps))+geom_point()+geom_smooth() +scale_x_log10()
#ggplot(foo1,aes(NbPassageKS, nps))+geom_point()+geom_smooth() +scale_x_log10()

#ggplot(foo1,aes(t, nps))+geom_point()+geom_smooth() +scale_x_log10()

foo1<- foo1%>%mutate(CaTicket=ifelse(CaTicket<0,0,CaTicket),
                     CaTicket_l=log10(CaTicket+1))

fit<- lm(nps~log(NbPassageKS)+log(CaTicket+1)+log(t+1), foo1)
summary(fit)
library(jtools)
effect_plot(fit, pred=CaTicket, interval = TRUE)#scale_x_log10()
effect_plot(fit, pred=NbPassageKS, interval = TRUE) #+scale_x_log10()
effect_plot(fit, pred=t, interval = TRUE)#scale_x_log10()

```


## Corpus

On utilise quanteda . 

```{r 04}
# 1 définir le corpus

corpus<-corpus(foo1,text_field ="explication")


# 2 tokeniser le corpus

toks <- tokens(corpus, remove_punct = TRUE, remove_numbers = TRUE) %>% 
    tokens_remove(pattern = stopwords("fr")) %>% 
  tokens_tolower()%>%
   tokens_remove(pattern="botanic.*")%>%
  tokens_group(groups=NPS)


# 3 construire le dfm 

dfm <- dfm(toks) %>%   
  dfm_trim(min_termfreq = 10, verbose = FALSE)

foo<-as.data.frame(dfm)

# 4 afficher le wordcloud

textplot_wordcloud(dfm,comparison = TRUE, color = col)

```

## Un autre wordcloud

Une autre méthode avec meilleure préparation du texte. et surtout de la collocation


pour le détail voir : https://quanteda.io/reference/textstat_collocations.html


```{r 05}

toks <- tokens(corpus, remove_punct = TRUE) %>% 
  tokens_tolower()%>%
  tokens_group(groups = NPS)

coloc <- textstat_collocations(toks, size = 2:4, min_count = 10) %>% filter(z>15)

head(coloc, 20)


toks2 <- tokens_compound(toks, pattern = coloc) %>%     
  tokens_remove(pattern = stopwords("fr") )



dfm <-toks2 %>%
    tokens_group(groups = NPS)%>% 
  dfm()

stat<- dfm %>% 
  textstat_frequency(n = 30,  groups = NPS)

g_b<-ggplot(stat, aes(label = feature)) +
  geom_text_wordcloud(aes(size=log(frequency), color=group)) +
  theme_minimal()+
  facet_wrap(vars(group))+
  scale_color_manual(values=col)+ 
  labs(title="Nuage des 50 mots les plus fréquents(Par groupes",
       caption = "La taille des mots est proportionnelle au log de leurs fréquences")
g_b
ggsave("NPS3.jpg", plot=last_plot(), width = 20, height = 20, units = "cm")

```

## Réseau sémantique


sur la base de 

https://kateto.net/networks-r-igraph/


```{r 05b}

toks2 <- tokens(corpus, remove_punct = TRUE, remove_numbers = TRUE) %>% 
    tokens_remove(pattern = stopwords("fr")) %>% 
  tokens_tolower()%>%
   tokens_remove(pattern="botanic.*", valuetype = "regex") 


fcmat <- fcm(toks2, context = "window",window = 5L, tri = FALSE)

feat <- names(topfeatures(fcmat, 500))

set.seed(100)
fcm_select(fcmat, pattern = feat) %>%
    textplot_network(min_freq = 50,edge_color = "grey50",
  edge_alpha = 0.2,vertex_labelsize=3)
```
    
    
## Pour comparer les segments, le Keyness index est particulièrement utile

Il est calculé en comparant un groupe cible à l'ensemble des autres groupes par une mesure d'association : un chi², ou un point.wise correlation.

```{r 06}

# Create a dfm per group
dfm <-toks2 %>%
    tokens_group(groups = NPS) %>% 
  dfm()


# Calculate keyness and determine "Promoteurs" as target group againts all other categories
result_keyness <- textstat_keyness(dfm, target = "Promoteurs")

# Plot estimated word keyness
g1<-textplot_keyness(result_keyness,   n = 30L, labelsize = 3,   show_legend = FALSE, 
                     show_reference = FALSE,   color = c("Darkgreen", "gray"))+
  xlim(-20,80) + 
  labs(x=NULL)


result_keyness <- textstat_keyness(dfm, target = "Détracteurs" )
g2<-textplot_keyness(result_keyness,   n = 30L, labelsize = 3,   show_legend = FALSE,   
                     show_reference = FALSE,   color = c("firebrick", "gray"))+
  xlim(0,80)+ 
  labs(x=NULL)


result_keyness <- textstat_keyness(dfm, target = "Passifs")
g3<-textplot_keyness(result_keyness,   n = 30L, labelsize = 3,   show_legend = FALSE,   show_reference = FALSE,    color = c("gold2", "gray"))+xlim(0,80)+ labs(x=NULL)



p<- plot_grid(g2, g3 ,g1,  labels = c('Détracteurs', 'Passifs', 'Promoteurs'), label_size = 12, ncol=3)
title <- ggdraw() + draw_label("NPS : Les raisons qui conduisent à la recommandation (keyness)", fontface='bold')
note <- ggdraw()+ draw_text("Les valeurs représentent le keyness des termes.\nIl mesure leur caractère distinctif par une statistique du chi²", size=8,x = 0.5, y = 0.5)


plot_grid(title, p,note, ncol=1, rel_heights=c(0.1, 1)) # rel_heights values control title margins


ggsave("NPS4.jpg", plot=last_plot(), width = 20, height = 20, units = "cm")
#  pour une comparaison deux à deux
#   pres_corpus <- corpus_subset(corpus, NPS %in% c("Détracteurs", "Promoteurs"))


#plot_grid(g ,p,d, labels = c("", "", "", ""), label_size = 12, ncol = 2, nrow = )

#ggsave("NPS5.jpg", plot=last_plot(), width = 20, height = 20, units = "cm")


```

## Un peu de topic analysis


La structure du modèle

!["modèle LDA"](Schematic-of-LDA-algorithm.png)


source : https://arxiv.org/pdf/1003.0783.pdf

preparation des données

```{r 07}
# pre processing : 
foo1<- foo1 %>%
  filter(!is.na(explication))

corpus<-corpus(foo1,text_field ="explication")

toks <- tokens(corpus, remove_punct = TRUE)

cols <- textstat_collocations(toks, size = 2:4, min_count = 20) %>% filter(z>20)

toks2 <- tokens_compound(toks, pattern = cols) %>%     
  tokens_remove(pattern = stopwords("fr") )

dfm<-dfm(toks2)
dfm<- dfm_trim(dfm, min_termfreq = 20, min_docfreq = 2)


```

estimation du modèle


```{r 07b, eval=FALSE}

#library(seededlda)
t1=Sys.time()

set.seed(123)
tmod_lda <- textmodel_lda(dfm, k = 7)

t2=Sys.time()
t=t2-t1
t

#lister les mots les plus associés
terms(tmod_lda, 25)

dfm$topic <- topics(tmod_lda)

#saveRDS(tmod_lda,"lda.rds")

#saveRDS(dfm,"topic.rds")
```

On sauvegarde les résultats pour se prémunir de l'instabilité des solutions ( en dépit du set seed) dans un format rds

## Analyse des topics


```{r 10}
tmod_lda<- readRDS("lda.rds")


phi<-as.data.frame(tmod_lda$phi)
phi$topic<-row.names(phi)
phi<-phi %>%
  gather(variable, value, -topic) %>% 
  group_by(topic)%>%
  mutate(rank=row_number(desc(value))) %>% filter(rank<20)

ggplot(phi, aes(x=topic, y=rank,label = variable)) + 
  scale_y_reverse() +
  geom_text(aes(color=topic,size=1000*value))+
  theme_minimal()+
  scale_color_hue()+
  guides(color="none",size="none")


```


## Une bonne vieille analyse des correspondances pour associer les segments aux thèmes du discours

pour plus de détails sur l'analyse des correspondances regarder par exemple [Anne B Dufour](https://pbil.univ-lyon1.fr/R/pdf/add2.pdf).



```{r 08}
dfm<-readRDS("topic.rds")
#un peu de recodage
table(dfm$topic)

#on recode pour une meilleure lecture
#dfm$topic2[dfm$topic=="topic1"]<-"Magasin"
#dfm$topic2[dfm$topic=="topic2"]<-"Achat"
#dfm$topic2[dfm$topic=="topic3"]<-"Cartedefid"
#dfm$topic2[dfm$topic=="topic4"]<-"bioplantes"
#dfm$topic2[dfm$topic=="topic5"]<-"Personnel"
#dfm$topic2[dfm$topic=="topic6"]<-"prix"
#dfm$topic2[dfm$topic=="topic7"]<-"Caisse"

#table(dfm$topic2)

#le tableau croisé

ca<- table(dfm$topic, dfm$NPS)
prop.table(ca, 2)

#library ("FactoMineR")
#library(factoextra)
res.ca <- CA (ca, graph = FALSE)
fviz_ca_biplot (res.ca, repel = TRUE)


```


## Régression gam pour mesurer les non-linéarités

elles sont non linéaires donc adaptées à détecter une structure type [kano](https://fr.wikipedia.org/wiki/Mod%C3%A8le_de_Kano) (pour certais attributs l'absence conduit à l'insatisfaction, mais leur présence ne conduit pas à plus d'information, et pour d'autres leur absence ne conduit pas à l'insatisfaction alors que leur présence renforce la satisfaction)

https://www.researchgate.net/publication/228434290_The_Kano_model-A_review_of_its_application_in_marketing_research_from_1984-2006/figures

Dans l'idée celà correspond à la théorie bifactorielle de herzberg.

Avec cette application on mesure la présence dans le discours d'un attribut. Ce degré de présence est simplement le paramètre theta du topic k pour texte i. Ce dont les consommateurs parlent peut a priori être aussi bien associés à une bonne ou une mauvaise opinion. Si la corrélation (linéaire est positive) c'est qu'on a affaire à un argument attractif qui ajoute aux attributs de base qui restent transparent pour le locuteur. Si elle est négative c'est qu'on à affaire à un une besoin de base, plus il pose problème et plus on en parle. D'autres attributs peuvent se caractériser par une courbe en U, il sont associé positivement quand on en parle modéremment, ou en compagnie d'un autre attribut. On observe pas la configuration d'un U inversé.

inspiré de https://rpubs.com/apierucci/lm-gam


```{r 09}
tmod_lda<- readRDS("lda.rds")


theta<-tmod_lda$theta

topic<-cbind(foo1,theta)

#on recode pour une meilleure lecture
#topic <- topic %>% 
#  rename(
#Caisse = topic1,
#Magasin = topic2,
#Bioplantes= topic3,
#Personnel=topic4,
#Cartedefid= topic5,
#Achat= topic6,
#Prix =topic7)

fit<-lm(nps~topic1+topic2+topic3+topic4+topic5 +topic6, data =topic)
summary(fit)

library(gam)
mod1 <- gam(nps ~ s(topic1) +s(topic2)+s(topic3)+s(topic4)+s(topic5)+s(topic6), data = topic, family = gaussian)

summary(mod1)

par(mfrow = c(2, 3))
termplot(mod1,rug = TRUE, se = TRUE,  terms = c("s(topic1)", "s(topic2)", "s(topic3)", "s(topic4)", "s(topic5)", "s(topic6)"))

```

## Segmenter


```{r 09}

df_clus<- topic %>% 
  select(topic1,topic2,topic3,topic4,topic5,topic6,topic7) %>%drop_na()

dist<-dist(df_clus)



fit <-hclust(dist, method="ward")

#afficher
plot(fit)


groups <- as.data.frame(cutree(fit, k=4)) 

df_clus<- cbind(df_clus, groups)  %>% rename(segment=8)


foo<- df_clus %>%
  gather(variable, value , -segment) %>%
  group_by(segment, variable) %>% 
  summarise(value=mean(value))# cut tree into 5 clusters

g1<- ggplot(foo, aes(x=segment,y=value, group=variable))+
  geom_bar(stat="identity", aes(fill= variable))

table(df_clus$segment)
g2<-ggplot(df_clus, aes(x=segment))+geom_bar()
g2

plot_grid(g1, g2,  nrow=2)

df_clus2<-cbind(topic,groups) %>%
  rename(segment=22)%>% 
  group_by(segment)%>%
  summarise(nps=mean(nps, na.rm=TRUE))



ggplot(df_clus2, aes(x=segment,y=nps))+geom_bar(stat="identity")


```

```{r 10}
df<-read.csv("nps.csv") %>% 
  select(explication, id) %>%
  filter(!is.na(explication))


library(cleanNLP)
cnlp_init_udpipe(model_name = "french")
annotate<-cnlp_annotate(df, text_name = "explication",doc_name = "id", verbose = 1000)

obj<-annotate$token
 
 
obj<-read_rds("annotationPOS_hotel.rds")

obj<-obj%>%mutate(nbcar=nchar(lemma))
 
 
# text2<-obj%>%filter(upos %in% c("ADJ","VERB","NOUN","PROPN"),nbcar>2)
# 
# summary(text2$nbcar)
# 
# text2<-text2%>%group_by(doc_id)%>%summarise(text=paste(lemma , collapse = " "))
# 
# df2<-inner_join(df2,text2, by=c("ID"="doc_id"))
```