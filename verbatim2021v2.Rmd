---
title: "Autour du NPS"
author: "cb"
date: "19/10/2021"
output: html_document
---


# les packages utilisés

```{r setup,warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE,include=TRUE, warning=FALSE, message=FALSE)
library(tidyverse)
library(readxl)
library(lubridate)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)

library(ggwordcloud)
library(tidytext)
library(cowplot)

library ("FactoMineR")
library(factoextra)


df<-readRDS("Enqueteclient.rds")

#palette de couleur
col<-c("firebrick","Gold3","Darkgreen")
```

## Les données

Quelques recodages

```{r data}
foo<-df %>% 
  select(2, 3, 4, 5, 7, 10, 11, 12, 13, 15,NumQuestion, Reponse)%>%
  mutate(nps=ifelse(NumQuestion==90,Reponse,NA),
         explication=ifelse(NumQuestion==94,Reponse,NA),
         satisfaction=ifelse(NumQuestion==88,Reponse,NA),
         intention=ifelse(NumQuestion==89,Reponse,NA),
         t2=year(datedebutadhesion),
         t1=year(DateEnvoiMail),
         t=t1-t2)%>%
  group_by(id)%>%
  summarise(nps=max(nps, na.rm=TRUE),
            satisfaction=max(satisfaction, na.rm=TRUE),
            intention=max(intention, na.rm=TRUE),
            explication=last(explication),
            NbPassageKS=last(NbPassageKS),
             CaTicket=last(CaTicket),
            t=last(t))

foo$CaTicket<-as.numeric(foo$CaTicket)


foo$nps<-as.numeric(foo$nps)
foo$satisfaction<-as.numeric(foo$satisfaction)
foo$intention<-as.numeric(foo$intention)
```

# Distribution du NPS

Notre tableau de données rend compte de 15000 évaluations en terme de NPS de la relation à une enseigne de jardinage. 


```{r 03}


foo1<-foo %>%
  mutate(NPS=ifelse(nps<7, "Détracteurs", 
                    ifelse(nps>6 & nps<9,"Passifs", "Promoteurs"))) %>% 
  mutate(p=ifelse(NPS=="Promoteurs", 1, 0),
         d=ifelse(NPS=="Détracteurs", 1, 0))

d<- mean(foo1$d, na.rm=TRUE)

p<-mean(foo1$p, na.rm=TRUE)

score_nps<- round((p-d)*100,1)



g<-ggplot(foo1, aes(x=nps))+
  geom_histogram(binwidth = 1,aes(fill=NPS))+
  theme_bw()+ 
  labs( title= " Distribution des scores NPS", subtitle = paste0("Score NPS = ", score_nps), caption = "n=13954", y = "Fréquence")+ scale_fill_manual(values=col)+
  scale_x_discrete(name="Note NPS", breaks=c("1","2","3","4","5","6","7", "8", "9", 10),
                   limits=c("1","2","3","4","5","6","7", "8", "9", 10))
g  
ggsave("NPS1.jpg", plot=last_plot(), width = 20, height = 20, units = "cm")

```



# Structure des corrélations

On peut examiner la corrélation de cet indicateurs à quelques autres variables : d'autres jugement, et les comportements antérieurs.



```{r 02}

foo2<-foo%>%select(2,3,4,6,CaTicket,t) %>% drop_na()
r<- round(cor(foo2),3)
library(ggcorrplot)
ggcorrplot(r, hc.order = TRUE, type = "lower",
   outline.col = "white",
   colors = c("#6D9EC1", "white", "#E46726"),
   lab=TRUE)
```

#  Un petit modèle linéaires

```{r cor}

fit<- lm(nps~NbPassageKS+CaTicket+t, foo)
summary(fit)
```


## Corpus

On utilise quanteda . 

```{r 04}

corpus<-corpus(foo1,text_field ="explication")
toks <- tokens(corpus, remove_punct = FALSE) %>% 
    tokens_remove(pattern = stopwords("fr"))%>%
   tokens_remove(pattern="Botanic.*") %>%
    tokens_group(groups = NPS)

dfm <- dfm(toks) %>%   
  dfm_trim(min_termfreq = 30, verbose = FALSE)

textplot_wordcloud(dfm,comparison = TRUE, color = col)


```

## Un autre wordcloud

autre méthode avec meilleure préparation du text


```{r 05}



toks <- tokens(corpus, remove_punct = TRUE) %>%
  tokens_group(groups = NPS)

cols <- textstat_collocations(toks, size = 2:4, min_count = 5) %>%filter(z>15)
cols
toks2 <- tokens_compound(toks, pattern = cols) %>%     
  tokens_remove(pattern = stopwords("fr") )



dfm <-toks2 %>%
    tokens_group(groups = NPS)%>% 
  dfm()

stat<- dfm %>% 
  textstat_frequency(n = 50,  groups = NPS)

g_b<-ggplot(stat, aes(label = feature)) +
  geom_text_wordcloud(aes(size=log(frequency), color=group)) +
  theme_minimal()+
  facet_wrap(vars(group))+scale_color_manual(values=col)+ 
  labs(title="Nuage des 50 mots les plus fréquents(Par groupes",
       caption = "La taille des mots est proportionnelle au log de leurs fréquences")
g_b
ggsave("NPS3.jpg", plot=last_plot(), width = 20, height = 20, units = "cm")

```

## pour comparer le Keyness index est particulièrement utile


```{r 06, fig.width=9}

# Create a dfm per group
dfm <-toks2 %>%
    tokens_group(groups = NPS)%>% 
  dfm()


# Calculate keyness and determine "Promoteurs" as target group againts all other categories
result_keyness <- textstat_keyness(dfm, target = "Promoteurs") %>% filter (n_target>20)
# Plot estimated word keyness
g1<-textplot_keyness(result_keyness,   n = 30L, labelsize = 3,   show_legend = FALSE, show_reference = FALSE,   color = c("Darkgreen", "gray"))+
  xlim(0,80) + 
  labs(x=NULL)


result_keyness <- textstat_keyness(dfm, target = "Détracteurs" )
g2<-textplot_keyness(result_keyness,   n = 30L, labelsize = 3,   show_legend = FALSE,   
                     show_reference = FALSE,   color = c("firebrick", "gray"))+
  xlim(0,80)+ 
  labs(x=NULL)


result_keyness <- textstat_keyness(dfm, target = "Passifs")
g3<-textplot_keyness(result_keyness,   n = 30L, labelsize = 3,   show_legend = FALSE,   show_reference = FALSE,    color = c("gold2", "gray"))+xlim(0,80)+ labs(x=NULL)

p<- plot_grid(g2, g3 ,g1,  labels = c('Détracteurs', 'Passifs', 'Promoteurs'), label_size = 12, ncol=3)
title <- ggdraw() + draw_label("NPS : Les raisons qui conduisent à la recommandation (keyness)", fontface='bold')
note <- ggdraw()+ draw_text("Les valeurs représentent le keyness des termes.\nIl mesure leur caractère distinctif par une statistique du chi²", size=8,x = 0.5, y = 0.5)
plot_grid(title, p,note, ncol=1, rel_heights=c(0.1, 1)) # rel_heights values control title margins


ggsave("NPS4.jpg", plot=last_plot(), width = 20, height = 20, units = "cm")
#  pour une comparaison deux à deux
#   pres_corpus <- corpus_subset(corpus, NPS %in% c("Détracteurs", "Promoteurs"))


plot_grid(g ,g_b,p,d, labels = c("", "", "", ""), label_size = 12, 
          ncol = 2, nrow = 2)

ggsave("NPS5.jpg", plot=last_plot(), width = 20, height = 20, units = "cm")


```

## un peu de topic analysis


```{r 07, fig.width=9}
# pre processing : 
toks <- tokens(corpus, remove_punct = TRUE)

cols <- textstat_collocations(toks, size = 2:4, min_count = 5) %>%filter(z>20)

toks2 <- tokens_compound(toks, pattern = cols) %>%     
  tokens_remove(pattern = stopwords("fr") ) 

dfm<-dfm(toks2)


library(seededlda)
set.seed(007)
tmod_lda <- textmodel_lda(dfm, k = 8)
#lister les mots les plus associés
terms(tmod_lda, 25)

```

## une bonne vieille analyse des correspondances

```{r 08, fig.width=9}
#un peu de recodage
dfm$topic <- topics(tmod_lda)
table(dfm$topic)

#on recode pour une meilleure lecture
dfm$topic2[dfm$topic=="topic1"]<-"conseil"
dfm$topic2[dfm$topic=="topic2"]<-"Carte de fid"
dfm$topic2[dfm$topic=="topic3"]<-"Caisse"
dfm$topic2[dfm$topic=="topic4"]<-"c'est bien continuez"
dfm$topic2[dfm$topic=="topic5"]<-"acheter"
dfm$topic2[dfm$topic=="topic6"]<-"Compétence amabilité"
dfm$topic2[dfm$topic=="topic7"]<-"prix"
dfm$topic2[dfm$topic=="topic8"]<-"choix des plantes"

table(dfm$topic2)

#le tableau croisé

ca<- table(dfm$topic2, dfm$NPS)
prop.table(ca, 2)

#library ("FactoMineR")
#library(factoextra)
res.ca <- CA (ca, graph = FALSE)
fviz_ca_biplot (res.ca, repel = TRUE)

```

